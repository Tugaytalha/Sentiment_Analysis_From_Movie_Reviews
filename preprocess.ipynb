{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing\n",
    "In this notebook, we will preprocess the data to make it ready for the model. The steps are as follows:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "516908cee519f34d"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-13T18:16:31.522575200Z",
     "start_time": "2024-06-13T18:16:31.509059300Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "import contractions\n",
    "from autocorrect import Speller"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0. Setting variables\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88a9e3eb793c9ddf"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Set the path to the dataset\n",
    "dataset_path = 'data/merged_data.csv'\n",
    "\n",
    "# Set the path to save the preprocessed dataset\n",
    "preprocessed_dataset_path = 'data/preprocessed_dataset.csv'\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-13T18:16:31.555900200Z",
     "start_time": "2024-06-13T18:16:31.528854400Z"
    }
   },
   "id": "fe90b8b3dac2c671"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Preparing the libraries "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "835cfd6bf26196a0"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Tugay Talha\n",
      "[nltk_data]     İçen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Tugay Talha\n",
      "[nltk_data]     İçen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Tugay Talha\n",
      "[nltk_data]     İçen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download stopwords for deleting unnecessary words\n",
    "nltk.download('stopwords')\n",
    "# Download lemmatizer for converting words to their base form\n",
    "nltk.download('wordnet')\n",
    "# Download spacy model for tokenization\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# Download punkt for tokenization\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "# Initialize lemmatizer, stopwords, and spell checker\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words = set(stopwords.words('english')) - {'not', 'no'}\n",
    "spell = Speller()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-13T18:16:32.144858Z",
     "start_time": "2024-06-13T18:16:31.539377300Z"
    }
   },
   "id": "5da9d0aa2e3bb462"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Preprocessing the given text\n",
    "The following steps are performed to preprocess the text:\n",
    "1. Lowercasing\n",
    "2. Removing URLs\n",
    "3. Removing email addresses\n",
    "4. Removing punctuation\n",
    "5. Removing numbers\n",
    "6. Correcting misspellings\n",
    "7. Removing extra whitespaces\n",
    "8. Tokenization\n",
    "9. Removing stopwords\n",
    "10. Lemmatization\n",
    "11. Joining tokens back to a single string\n",
    "\n",
    "These steps are purify the text from unnecessary information that does not contribute to the meaning of the text."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc99e9831a5dfcce"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Expand contractions\n",
    "    text = contractions.fix(text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Remove punctuation (excluding apostrophes for contractions)\n",
    "    text = re.sub(r\"[^\\w\\s']\", '', text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Correct misspellings\n",
    "    text = spell(text)\n",
    "    \n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords (excluding 'not' and 'no') and lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Join tokens back to a single string\n",
    "    cleaned_text = ' '.join(tokens)\n",
    "    \n",
    "    return cleaned_text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-13T18:16:32.160607200Z",
     "start_time": "2024-06-13T18:16:32.150375100Z"
    }
   },
   "id": "a54a6a76ace5bc33"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Example of preprocessing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f7870a7e3222847"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not like movie terrible', 'great movie loved', 'check website', 'not sure enjoyed play bit long', 'product good service not', 'done fantastic job highly recommended', 'okay not best seen', 'wow incredible performance', 'would not buy waste money', 'email information', 'visit u detail', 'food great waiter rude', 'amazing concert last night not wait next one', 'gh hated ending book', 'believe happened latest episode']\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text_data = [\n",
    "    \"I don't like this movie! It's terrible.\", \n",
    "    \"Great movie! I loved it.\", \n",
    "    \"Check out this website: https://example.com\",\n",
    "    \"I'm not sure if I enjoyed the play; it was a bit too long.\",\n",
    "    \"The product is good, but the service is not.\",\n",
    "    \"They've done a fantastic job! Highly recommended.\",\n",
    "    \"It's okay, not the best I've seen.\",\n",
    "    \"Wow! What an incredible performance!\",\n",
    "    \"I wouldn't buy this again; it's a waste of money.\",\n",
    "    \"Email me at example@example.com for more information.\",\n",
    "    \"Visit us at http://www.example.com for details.\",\n",
    "    \"The food was great, but the waiter was rude.\",\n",
    "    \"Amazing concert last night! Can't wait for the next one.\",\n",
    "    \"Ugh, I hated the ending of that book.\",\n",
    "    \"Can you believe what happened in the latest episode?\"\n",
    "]\n",
    "preprocessed_data = [preprocess_text(text) for text in text_data]\n",
    "\n",
    "print(preprocessed_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-13T18:16:32.186706200Z",
     "start_time": "2024-06-13T18:16:32.164604400Z"
    }
   },
   "id": "c969a6d21fcc70dc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Loading the dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c48a6453b04df8"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "dataset = pd.read_csv(dataset_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-13T18:16:32.844648400Z",
     "start_time": "2024-06-13T18:16:32.177190600Z"
    }
   },
   "id": "55b8bd76017ccea1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Preprocessing the dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e2591509d8023d1"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# Preprocess the text data\n",
    "dataset['text'] = dataset['text'].apply(preprocess_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-13T20:53:34.102553200Z",
     "start_time": "2024-06-13T18:16:32.847649800Z"
    }
   },
   "id": "2eaced1207632338"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Saving the preprocessed dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "afad964bdd414b52"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# Save the preprocessed dataset\n",
    "dataset.to_csv(preprocessed_dataset_path, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-13T20:53:36.258476400Z",
     "start_time": "2024-06-13T20:53:34.102553200Z"
    }
   },
   "id": "d6a452b6a49d2bd2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "df49832367a38c4e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
